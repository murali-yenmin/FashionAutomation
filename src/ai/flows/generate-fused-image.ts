// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview This file defines a Genkit flow for fusing a product image onto a person's image,
 * placing them in a specified or default background.
 *
 * - generateFusedImage - The main function to initiate the image fusion process.
 * - GenerateFusedImageInput - The input type for the generateFusedImage function.
 * - GenerateFusedImageOutput - The return type for the generateFusedImage function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import wav from 'wav';

const GenerateFusedImageInputSchema = z.object({
  backgroundImageDataUri: z
    .string()
    .optional()
    .describe(
      "A background image as a data URI. If not provided, a default white background will be used. Must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  personImageDataUri: z
    .string()
    .describe(
      "An image of a person as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  productImageDataUri: z
    .string()
    .describe(
      "An image of a product (e.g., clothing) as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateFusedImageInput = z.infer<typeof GenerateFusedImageInputSchema>;

const GenerateFusedImageOutputSchema = z.object({
  fusedImageDataUri: z
    .string()
    .describe('The fused image as a data URI in base64 format.'),
});
export type GenerateFusedImageOutput = z.infer<typeof GenerateFusedImageOutputSchema>;

export async function generateFusedImage(input: GenerateFusedImageInput): Promise<GenerateFusedImageOutput> {
  return generateFusedImageFlow(input);
}

const generateFusedImageFlow = ai.defineFlow(
  {
    name: 'generateFusedImageFlow',
    inputSchema: GenerateFusedImageInputSchema,
    outputSchema: GenerateFusedImageOutputSchema,
  },
  async input => {
    const {
      backgroundImageDataUri,
      personImageDataUri,
      productImageDataUri,
    } = input;

    // Construct the prompt.  The model being used here is googleai/gemini-2.5-flash-image-preview, so
    // we must pass TEXT and IMAGE in the prompt in the right order and with the right structure.
    const prompt = [
      {media: {url: personImageDataUri}},
      {
        text: `Instructions: Use the product image to modify the person image such that the product is realistically fitted onto the person. If the background image is provided, composite the result into the background. The final result should look realistic.`,
      },
      {media: {url: productImageDataUri}},
    ];

    if (backgroundImageDataUri) {
      prompt.push({
        text: `Composite with this background image.`,
      });
      prompt.push({media: {url: backgroundImageDataUri}});
    } else {
      prompt.push({
        text: `Since no background was provided, generate a white background.`,
      });
    }

    const {media} = await ai.generate({
      model: 'googleai/gemini-2.5-flash-image-preview',
      prompt,
      config: {
        responseModalities: ['TEXT', 'IMAGE'], // MUST provide both TEXT and IMAGE, IMAGE only won't work
      },
    });

    if (!media) {
      throw new Error('No fused image was generated.');
    }

    return {fusedImageDataUri: media.url};
  }
);
